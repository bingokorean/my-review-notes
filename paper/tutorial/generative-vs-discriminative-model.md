# Generative model vs. Discriminative Model

2015.10

## 1. Motivation

```
Example )  (1, 0), (1, 0), (2, 0), (2, 1)
Input data : x
Goal : to classify the data x into labels y

A generative model learns the joint probability distribution p(x, y)

   p(x, y)   y=0    y=1
     x=1     1/2     0
     x=2     1/4    1/4

sample size = the whole population
pdf of x and y

A discriminative model learns the conditional probability distribution p(y| x)

   p(y |x)   y=0    y=1
     x=1      1      0
     x=2     1/2    1/2

sample size = all cases of x=1 or x=2
pdf of y
```

## 2. Comparison

### 2.1 Generative Model

A generative model is a model for randomly generating observable-data values, typically given some hidden parameters. It specifies a joint probability distribution over observation and label sequences. Generative models are used in machine learning for either modeling data directly that is, modeling observations drawn from a probability density function, or as as intermediate step to forming a conditional probability density function. A conditional distribution can be formed from a generative model through Bayes' rule.

Generative models contrast with discriminative models, in that a generative model is a full probabilistic model of all variables, whereas a discriminative model provides a model only for the target variable conditional on the observed variables. Thus a generative model can be used to generate values of any variable in the model, whereas a discriminative model allows only sampling of the target variables conditional on the observed quantities.

To predict the label y from the training data x, you must evaluate:

f(x) = argmax_y(p(x|y)p(y))

which we had the joint probability distribution p(x, y) because p(x, y) = p(x |y)*p(y), which explicitly models the actual distribution of each class.

Generative models model the distribution of individual classes.

There are a number of advantages generative models may offer, depending on the application. Say you are dealing with non-stationary distributions, where the online test data may be generated by more different underlying distributions than training data. It's typically more straightforward to detect distribution changes and update a generative model accordingly than do this for a decision boundary in an SVM, especially if the online updates need to be unsupervised. Discriminative models also don't generally function for outlier detection, though generative models generally do. What's best for a specific application should be evaluated based on the application.

Generative models are typically specified as probabilistic graphical models, which offer rich representations of the independence relations in the dataset.

A generative model is a model of how the data is actually generated. For example, Gaussian mixture models have a nice probabilistic model for how points are generated ( each data points are sampled from component's Gaussian distribution )

Generative algorithms have discriminative properties because you can get p(y |x) once you have p(x |y) and p(y) by Bayes' Theorem, though discriminative algorithms don't really have generative properties.

Generative algorithms make some kind of structure assumptions on your model, but discriminative algorithms make fewer assumptions. For example, Naive Bayes assumes conditional independence of your features, while logistic regression does not.

Naive Bayes is generative because it captures p(x |y) and p(y). For example, if we know that p(y=English)=0.7 and p(y=French)=0.3 along with English and French word distributions, then we can now generate a new document by first choosing the language of the document, p(y), and then generating words from its word distributions.

Generative model은 p(y)와 p(x |y)를 가지고 Bayes rule을 통해 p(y |x)로 돌아갈 수 있지만, Discriminative model은 오직 p(y |x)에 머무를수밖에 없다.

Generative models often outperform discriminative models on smaller datasets because their generative assumptions place some structure on your model that prevent overfitting. The Naive Bayes assumption is rarely satisfied, so logistic regression will tend to outperform Naive Bayes as your dataset grows because it can capture dependencies that Naive Bayes can't. But when you only have a small dataset, logistic regression might pick up on spurious patterns that don't really exist. Therefore, the Naive Bayes acts as a kind of regularizer on your model that prevents overfitting.

Generative models can actually learn the underlying structure of the data if you specify your model correctly and the model actually holds, but discriminative models can outperform in case that your generative assumptions are not satisfied because discriminative algorithms are less tied to a particular structure, and the real world is messy and assumptions are rarely perfectly satisfied anyways.





