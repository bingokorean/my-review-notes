{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polarity Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Movie Review Polarity Dataset (review polarity.tar.gz, 3MB). <br>\n",
    "http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. CNN with Embedding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Vocabulary Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read() # read all\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def clean_doc_for_voca(doc):\n",
    "    tokens = doc.split()\n",
    "    \n",
    "    # remove punctuations\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    \n",
    "    # remove non-alphabetic tokens\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    \n",
    "    # remove stop-words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if not word in stop_words]\n",
    "    \n",
    "    # remove non-freq words (주의: document범위내에서만으로 한정)\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore a single example and cleaned one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quaid stars as a man who has taken up the proffesion of dragonslayer after he feels he is betrayed by a dragon early in the movie . \n",
      "he runs into the last dragon in existence , and there is a genuinely amusing battle between the two which results in a standoff where quaid is in the dragons mouth , but has his sword pointed at the dragons brain . \n",
      "eventually , they decide to call a truce , and they work out a deal . \n",
      "since he is the last dragon , he will pretend to die and quaid will be able to get paid for it . \n",
      "their scam works at first , until they come to a town without any money . \n",
      "instead the town sacrifices a girl to the dragon , but of course , draco is a nice droagon , so he won't eat her . there is however a very amusing scene where draco is hitting on the young girl . \n",
      "of course , as you can probably tell by the plot , this is a silly movie , but it does know when to take itself seriously at the right times , unlike eddie , which was serious all the time . \n",
      "you could probably call this a medieval comedy , because there are more laughs here than in eddie and spy hard combined . \n",
      "dennis quaid makes a fine hero . \n",
      "pete posthlewaite provides some ghreat comedy as a monk who journeys with them . \n",
      "dina meyer is appealing as the sacrificed girl . \n",
      "but lets face it , the movie is really about the dragon , and what an amazing creation he is . \n",
      "connery's voice and ilm team up to provide us with a truly magnificent dragon . \n",
      "so , if you are going to see this movie for a strong hard core medieval epic , you are going to the wrong movie . \n",
      "if you are going because of the dragon , you will not be dissapointed , and you will be provided with plenty of laughs that smooth out the boring parts in the script . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the document\n",
    "filename = 'txt_sentoken/pos/cv132_5618.txt'\n",
    "text = load_doc(filename)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quaid', 'stars', 'man', 'taken', 'proffesion', 'dragonslayer', 'feels', 'betrayed', 'dragon', 'early', 'movie', 'runs', 'last', 'dragon', 'existence', 'genuinely', 'amusing', 'battle', 'two', 'results', 'standoff', 'quaid', 'dragons', 'mouth', 'sword', 'pointed', 'dragons', 'brain', 'eventually', 'decide', 'call', 'truce', 'work', 'deal', 'since', 'last', 'dragon', 'pretend', 'die', 'quaid', 'able', 'get', 'paid', 'scam', 'works', 'first', 'come', 'town', 'without', 'money', 'instead', 'town', 'sacrifices', 'girl', 'dragon', 'course', 'draco', 'nice', 'droagon', 'wont', 'eat', 'however', 'amusing', 'scene', 'draco', 'hitting', 'young', 'girl', 'course', 'probably', 'tell', 'plot', 'silly', 'movie', 'know', 'take', 'seriously', 'right', 'times', 'unlike', 'eddie', 'serious', 'time', 'could', 'probably', 'call', 'medieval', 'comedy', 'laughs', 'eddie', 'spy', 'hard', 'combined', 'dennis', 'quaid', 'makes', 'fine', 'hero', 'pete', 'posthlewaite', 'provides', 'ghreat', 'comedy', 'monk', 'journeys', 'dina', 'meyer', 'appealing', 'sacrificed', 'girl', 'lets', 'face', 'movie', 'really', 'dragon', 'amazing', 'creation', 'connerys', 'voice', 'ilm', 'team', 'provide', 'us', 'truly', 'magnificent', 'dragon', 'going', 'see', 'movie', 'strong', 'hard', 'core', 'medieval', 'epic', 'going', 'wrong', 'movie', 'going', 'dragon', 'dissapointed', 'provided', 'plenty', 'laughs', 'smooth', 'boring', 'parts', 'script']\n"
     ]
    }
   ],
   "source": [
    "tokens = clean_doc_for_voca(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "    doc = load_doc(filename)\n",
    "    tokens = clean_doc_for_voca(doc)\n",
    "    vocab.update(tokens) # vocab이 Counter()이므로.. (주의) Counter의 입력은 list형태이다.\n",
    "    \n",
    "def process_docs_for_voca(directory, vocab): # 데이터 파일이 폴더 안에 여러개가 있으므로...\n",
    "    for filename in listdir(directory):\n",
    "        if filename.startswith('cv9'):\n",
    "            continue\n",
    "        path = directory + '/' + filename\n",
    "        add_doc_to_vocab(path, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "document를 represent해야 되는데 vocabulary size를 어떻게 설정하느냐가 성능에 큰 영향을 끼친다 (최대한 상관성이 없는 단어들은 제외해야 한다) it is important to constrain the words to only those believed to be predictive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44276\n",
      "[('film', 7983), ('one', 4946), ('movie', 4826), ('like', 3201), ('even', 2262), ('good', 2080), ('time', 2041), ('story', 1907), ('films', 1873), ('would', 1844), ('much', 1824), ('also', 1757), ('characters', 1735), ('get', 1724), ('character', 1703), ('two', 1643), ('first', 1588), ('see', 1557), ('way', 1515), ('well', 1511), ('make', 1418), ('really', 1407), ('little', 1351), ('life', 1334), ('plot', 1288), ('people', 1269), ('could', 1248), ('bad', 1248), ('scene', 1241), ('movies', 1238)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocab = Counter()\n",
    "# add all docs in the folder to vocab\n",
    "process_docs_for_voca('txt_sentoken/pos', vocab)\n",
    "process_docs_for_voca('txt_sentoken/neg', vocab)\n",
    "\n",
    "print(len(vocab))\n",
    "print(vocab.most_common(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data cleaning 파트에서는 document 범위내에서 한정해서 빈도수가 낮은 단어들을 삭제했지만, vocabulary를 구축하고 난 다음에는 전체 corpus 범위내에서 빈도수가 낮은 단어들을 삭제할 수 있다. (따라서 2가지 범위에 따라 빈도수가 낮은 단어들을 삭제할 수 있다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25767\n"
     ]
    }
   ],
   "source": [
    "min_occurance = 2 \n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurance]\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vocab(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "# save vocab\n",
    "#save_vocab(tokens, 'voca.txt')\n",
    "\n",
    "# 여기선 save하지않고 그대로 계속 이어가자\n",
    "vocab = set(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "def clean_doc(doc, vocab):\n",
    "    tokens = doc.split()\n",
    "    # remove punctuations\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # remove tokens not in vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens\n",
    "\n",
    "def process_doc_with_split(directory, vocab, is_train):\n",
    "    documents = list()\n",
    "    for filename in listdir(directory):\n",
    "        # 데이터가 cv001~cv999까지 존재하는데, cv9xx 이상부터는 test set으로 사용하자 (10%)\n",
    "        if is_train and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_train and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        path = directory + '/' + filename\n",
    "        doc = load_doc(path)\n",
    "        tokens = clean_doc(doc, vocab)\n",
    "        documents.append(tokens)\n",
    "    return documents\n",
    "\n",
    "from numpy import array\n",
    "def load_clean_dataset(vocab, is_train):\n",
    "    neg = process_doc_with_split('txt_sentoken/neg', vocab, is_train)\n",
    "    pos = process_doc_with_split('txt_sentoken/pos', vocab, is_train)\n",
    "    docs = neg + pos\n",
    "    labels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
    "    return docs, labels\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines) # fit_on_text함수의 입력은 list of texts이다.\n",
    "    return tokenizer\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "def encode_docs(tokenizer, max_length, docs):\n",
    "    encoded = tokenizer.texts_to_sequences(docs)\n",
    "    padded = pad_sequences(encoded, maxlen=max_length, padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 25768\n"
     ]
    }
   ],
   "source": [
    "## Load vocabulary\n",
    "#vocab_filename = 'voca.txt'\n",
    "#vocab = load_doc(vocab_filename)\n",
    "#vocab = set(vocab.split())\n",
    "\n",
    "## Load training data\n",
    "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
    "\n",
    "## Define tokenizer\n",
    "tokenizer = create_tokenizer(train_docs)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary size: %d' % vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tokenizer.word_index)\n",
    "\n",
    "# {'woods': 1322,\n",
    "#  'spiders': 13894,\n",
    "#  'darryls': 19605,\n",
    "#  'hanging': 2036,\n",
    "#  'woody': 924,\n",
    "#  'comically': 7910,\n",
    "#  'scold': 19606,\n",
    "#  'originality': 2136,\n",
    "#  'rickman': 7005,\n",
    "#  'bringing': 1454,\n",
    "#  'liaisons': 8492,\n",
    "#  'sommerset': 13895,\n",
    "#  'wooden': 2608,\n",
    "#  'wednesday': 12257,\n",
    "#  'circuitry': 16193,\n",
    "#  'crotch': 8493,\n",
    "#  'elgar': 19607,\n",
    "# ...\n",
    "\n",
    "# 위와 같이 tokenizer는 거치면 word index를 가지게 된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length: 1317\n"
     ]
    }
   ],
   "source": [
    "## Encode data\n",
    "max_length = max([len(s.split()) for s in train_docs]) # 하나의 document에 최대 word 개수 설정\n",
    "print('Maximum length: %d' % max_length)\n",
    "Xtrain = encode_docs(tokenizer, max_length, train_docs)\n",
    "# encode_docs 함수를 통해 sequence words를 sequence voca index로 변환해준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1317, 100)         2576800   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 1310, 32)          25632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 655, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 20960)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                209610    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 2,812,053\n",
      "Trainable params: 2,812,053\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## DEFINE MODEL\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "def define_model(vocab_size, max_length):\n",
    "    model = Sequential()\n",
    "    embedding_dim = 100\n",
    "    model.add(Embedding(vocab_size, embedding_dim, input_length=max_length))\n",
    "    model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    #plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model\n",
    "\n",
    "model = define_model(vocab_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1800/1800 [==============================] - 15s 9ms/step - loss: 0.6902 - acc: 0.5539\n",
      "Epoch 2/10\n",
      "1800/1800 [==============================] - 15s 8ms/step - loss: 0.5067 - acc: 0.8161\n",
      "Epoch 3/10\n",
      "1800/1800 [==============================] - 15s 9ms/step - loss: 0.0857 - acc: 0.9861\n",
      "Epoch 4/10\n",
      "1800/1800 [==============================] - 16s 9ms/step - loss: 0.0088 - acc: 1.0000\n",
      "Epoch 5/10\n",
      "1800/1800 [==============================] - 16s 9ms/step - loss: 0.0030 - acc: 1.0000\n",
      "Epoch 6/10\n",
      "1800/1800 [==============================] - 15s 8ms/step - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 7/10\n",
      "1800/1800 [==============================] - 15s 8ms/step - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 8/10\n",
      "1800/1800 [==============================] - 16s 9ms/step - loss: 9.8533e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      "1800/1800 [==============================] - 16s 9ms/step - loss: 8.0487e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      "1800/1800 [==============================] - 15s 8ms/step - loss: 6.7797e-04 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bea9515a20>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TRAIN MODEL\n",
    "model.fit(Xtrain, ytrain, epochs=10, verbose=1)\n",
    "#model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 100.000000\n",
      "Test Accuracy: 87.500000\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# load trian data\n",
    "#train_docs, ytrain = load_clean_dataset(vocab, True)\n",
    "\n",
    "# load test data\n",
    "# 주의할 점은 (word, index) 정보가 있는 tokenizer는 train data와 test data 모두 똑같다\n",
    "test_docs, ytest = load_clean_dataset(vocab, False)\n",
    "Xtest = encode_docs(tokenizer, max_length, test_docs)\n",
    "\n",
    "# load pre-trained model\n",
    "#model = load_model('model.h5')\n",
    "\n",
    "_, acc1 = model.evaluate(Xtrain, ytrain, verbose=0)\n",
    "print('Train Accuracy: %f' % (acc1*100))\n",
    "\n",
    "_, acc2 = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc2*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: [Everything will enjoy this film. I love it, recommended!]\n",
      "Sentiment: NEGATIVE (51.988%)\n",
      "\n",
      "\n",
      "Review: [This is a bad movie. Do not watch it. It sucks.]\n",
      "Sentiment: NEGATIVE (54.339%)\n"
     ]
    }
   ],
   "source": [
    "def predict_sentiment(review, vocab, tokenizer, max_length, model):\n",
    "    line = clean_doc(review, vocab)\n",
    "    #print(line)\n",
    "    padded = encode_docs(tokenizer, max_length, [line])\n",
    "    #print(padded)\n",
    "    #print(len(padded[0]))\n",
    "    yhat = model.predict(padded, verbose=0)\n",
    "    percent_pos = yhat[0,0]\n",
    "    if round(percent_pos)==0: # 0.5이하라면..\n",
    "        return (1-percent_pos), 'NEGATIVE'\n",
    "    return percent_pos, 'POSITIVE'\n",
    "\n",
    "text = 'Everything will enjoy this film. I love it, recommended!'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, max_length, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))\n",
    "\n",
    "print('\\n')\n",
    "text = 'This is a bad movie. Do not watch it. It sucks.'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, max_length, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More things to do\n",
    "* better data cleaning\n",
    "* truncated senquences (maybe shorter max_length?)\n",
    "* truncated vocabulary (maybe smaller voca_size?)\n",
    "* better cnn architecture (fileters and kernel size, depth, width)\n",
    "* better optimization policy (epochs and batch size)\n",
    "* pretrained word embedding\n",
    "* divide long and short reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. n-gram CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* text classification을 위한 가장 간단한 딥러닝 모델은 word embedding layer와 one-dimensional convolutional neural network를 사용하는 것이었다. \n",
    "* different kernel size를 사용하는 multiple parallel convolutional neural networks로 확장할 수 있다. \n",
    "* 이러한 모델은 multichannel convolutional neural network와 같고 different n-gram sizes (groups of words)를 읽을 수 있다고 볼 수 있다\n",
    "* This allows the document to be processed at different resolutions or different n-grams (groups of words) at a time, whilst the model learns how to best integrate these interpretations.\n",
    "* Yoon Kim 2014 paper (여기서는 embedding은 안 건드리고 different kernel size만 사용(multichannel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save dataset\n",
    "# def save_dataset(dataset, filename):\n",
    "#     dump(dataset, open(filename, 'wb'))\n",
    "#     print('Saved: %s' % filename)\n",
    "\n",
    "# # load dataset\n",
    "# def load_dataset(filename):\n",
    "#     return load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train n-gram CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_16 (InputLayer)           (None, 1317)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_17 (InputLayer)           (None, 1317)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           (None, 1317)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_17 (Embedding)        (None, 1317, 100)    2576800     input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_18 (Embedding)        (None, 1317, 100)    2576800     input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_19 (Embedding)        (None, 1317, 100)    2576800     input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 1314, 32)     12832       embedding_17[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 1312, 32)     19232       embedding_18[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 1310, 32)     25632       embedding_19[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 1314, 32)     0           conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 1312, 32)     0           conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 1310, 32)     0           conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 657, 32)      0           dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 656, 32)      0           dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 655, 32)      0           dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_17 (Flatten)            (None, 21024)        0           max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_18 (Flatten)            (None, 20992)        0           max_pooling1d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_19 (Flatten)            (None, 20960)        0           max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 62976)        0           flatten_17[0][0]                 \n",
      "                                                                 flatten_18[0][0]                 \n",
      "                                                                 flatten_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 10)           629770      concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 1)            11          dense_11[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 8,417,877\n",
      "Trainable params: 8,417,877\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers import Dropout\n",
    "\n",
    "def define_n_gram_model(length, vocab_size):\n",
    "    # channel 1\n",
    "    inputs1 = Input(shape=(length,))\n",
    "    embedding1 = Embedding(vocab_size, 100)(inputs1)\n",
    "    conv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
    "    drop1 = Dropout(0.5)(conv1)\n",
    "    pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "    flat1 = Flatten()(pool1)\n",
    "    # channel 2\n",
    "    inputs2 = Input(shape=(length,))\n",
    "    embedding2 = Embedding(vocab_size, 100)(inputs2)\n",
    "    conv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n",
    "    drop2 = Dropout(0.5)(conv2)\n",
    "    pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "    flat2 = Flatten()(pool2)    \n",
    "    # channel 3\n",
    "    inputs3 = Input(shape=(length,))\n",
    "    embedding3 = Embedding(vocab_size, 100)(inputs3)\n",
    "    conv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n",
    "    drop3 = Dropout(0.5)(conv3)\n",
    "    pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "    flat3 = Flatten()(pool3)    \n",
    "    \n",
    "    # merge\n",
    "    merged = concatenate([flat1, flat2, flat3])\n",
    "    # interpretation\n",
    "    dense1 = Dense(10, activation='relu')(merged)\n",
    "    outputs = Dense(1, activation='sigmoid')(dense1)\n",
    "    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
    "    # compile\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize\n",
    "    model.summary()\n",
    "    #plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
    "    return model\n",
    "\n",
    "model = define_n_gram_model(max_length, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "1800/1800 [==============================] - 49s 27ms/step - loss: 0.6890 - acc: 0.5267\n",
      "Epoch 2/7\n",
      "1800/1800 [==============================] - 45s 25ms/step - loss: 0.4271 - acc: 0.7750\n",
      "Epoch 3/7\n",
      "1800/1800 [==============================] - 46s 26ms/step - loss: 0.0427 - acc: 0.9883\n",
      "Epoch 4/7\n",
      "1800/1800 [==============================] - 46s 26ms/step - loss: 0.0035 - acc: 1.0000\n",
      "Epoch 5/7\n",
      "1800/1800 [==============================] - 45s 25ms/step - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 6/7\n",
      "1800/1800 [==============================] - 44s 25ms/step - loss: 7.7017e-04 - acc: 1.0000\n",
      "Epoch 7/7\n",
      "1800/1800 [==============================] - 47s 26ms/step - loss: 4.9838e-04 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1beaf11ec88>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([Xtrain, Xtrain, Xtrain], ytrain, epochs=7, batch_size=16)\n",
    "#model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test n-gram CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 100.000000\n",
      "Test Accuracy: 88.500000\n"
     ]
    }
   ],
   "source": [
    "_, acc1 = model.evaluate([Xtrain, Xtrain, Xtrain], ytrain, verbose=0)\n",
    "print('Train Accuracy: %f' % (acc1*100))\n",
    "\n",
    "_, acc2 = model.evaluate([Xtest, Xtest, Xtest], ytest, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc2*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More things to do...\n",
    "* different n-grams (change the kernel size)\n",
    "* more or fewer channels\n",
    "* shared embedding (다수의 채널에 똑같은 word embedding 사용)\n",
    "* 다른 pre-train word embedding 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
