{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS tagger with Viterbi algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "* [1] [한국어 형태소 분석기와 품사 태거 구현](https://github.com/gritmind/morph_and_pos_analyzer_korean)\n",
    "* [2] [A deep dive into part-of-speech tagging using the Viterbi algorithm](https://medium.freecodecamp.org/a-deep-dive-into-part-of-speech-tagging-using-viterbi-algorithm-17c8de32e8bc)\n",
    "* [3] [Viterbi matrix for calculating the best POS tag sequence of a HMM POS tagger](https://www.youtube.com/watch?v=_568XqOByTs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################\n",
    "\"\"\" Load Resources  and Input Data \"\"\"\n",
    "#####################################################################################################\n",
    "# Resoruces (i.e. transition probabilities, output probabilities)\n",
    "# Input Data (i.e. multiple sentences to be tagged)\n",
    "\n",
    "# resources for test\n",
    "trans_prob_dict = {\n",
    "    ('Noun','Noun'):      1 * 10 ** -1, \n",
    "    ('Verb','Noun'):      4 * 10 ** -1, \n",
    "    ('Verb','Adv'):       4 * 10 ** -1, \n",
    "    ('Noun','Adv'):       1 * 10 ** -1, \n",
    "    ('Noun','Verb'):      3 * 10 ** -1, \n",
    "    ('Verb', 'Verb'):     1 * 10 ** -1, \n",
    "    ('<s>','Noun'):       2 * 10 ** -1, \n",
    "    ('<s>','Verb'):       3 * 10 ** -1, \n",
    "    ('Adj','</s>'):       1 * 10 ** -1\n",
    "}\n",
    "output_prob_dict = {\n",
    "    ('Verb','learning'):      3 * 10 ** -3, \n",
    "    ('Verb','changes'):       4 * 10 ** -3, \n",
    "    ('Adv','thoroughly'):     2 * 10 ** -3, \n",
    "    ('Noun','learning'):      1 * 10 ** -3,\n",
    "    ('Noun','changes'):       3 * 10 ** -3\n",
    "}\n",
    "\n",
    "# input data for test\n",
    "input_word_list = [\"learning changes thoroughly\", \"learning changes\", \"learning\"]\n",
    "\n",
    "\n",
    "# extract more information from resources\n",
    "set_tag_list = [key[0] for key in list(trans_prob_dict.keys())] + [key[1] for key in list(trans_prob_dict.keys())]\n",
    "set_tag_list = list(set(set_tag_list))\n",
    "len_set_tag_list = len(set_tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\t learning changes thoroughly\n",
      "Output:\t Verb Verb Adv\n",
      "-----------------------------\n",
      "Input:\t learning changes\n",
      "Output:\t Verb Noun\n",
      "-----------------------------\n",
      "Input:\t learning\n",
      "Output:\t Verb\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "#####################################################################################################\n",
    "\"\"\" Viterbi Algorithm \"\"\"\n",
    "#####################################################################################################\n",
    "# 통합확률은 Hidden Markov Model에 의해서 정의되어, 전이확률과 출력확률로 구성됨.\n",
    "# 단어와 태그 사이의 모든 경우에 해당하는 전이확률과 출력확률을 미리 구해야함.\n",
    "smoothing = 1e-9\n",
    "\n",
    "def ExtractMax_Prob_Index(total_prob_state, i, j, trans_prob_dict, output_prob_state):\n",
    "    \n",
    "    temp = [0 for x in range(len(set_tag_list))]\n",
    "    for k, _ in enumerate(set_tag_list):\n",
    "\n",
    "        try:\n",
    "            cur_trans_prob = trans_prob_dict[(set_tag_list[k], set_tag_list[j])]\n",
    "        except:\n",
    "            cur_trans_prob = smoothing\n",
    "        \n",
    "        ## 통합확률 bottom-up process - 현재(2D-state)에서의 모든 통합확률을 구한다.  \n",
    "        temp[k] = total_prob_state[i-1][k] * cur_trans_prob * output_prob_state[i][j]  # index 주의, log-sum 고려\n",
    "    \n",
    "    # 가장 높은 확률을 가지는 과거의 태그축-state index를 추출한다.\n",
    "    max_prob = max(temp)\n",
    "    argmax_idx = temp.index(max_prob)\n",
    "    return max_prob, argmax_idx\n",
    "\n",
    "\n",
    "def ViterbiAlgorithm(splited_sent):\n",
    "    \n",
    "    ## STEP0 - 준비 ([x=단어,y=태그] 라는 2D-State 공간을 구성)\n",
    "    # 출력확률, 통합확률, 이전index(backtracking때 필요)를 저장할 수 있는 3가지 종류의 2D-State를 만듦.    \n",
    "    len_splited_sent = len(splited_sent)\n",
    "    output_prob_state = [[0 for x in range(len_set_tag_list)] for y in range(len_splited_sent)]\n",
    "    total_prob_state = [[0 for x in range(len_set_tag_list)] for y in range(len_splited_sent)]\n",
    "    total_prev_state_idx = [[-1 for x in range(len_set_tag_list)] for y in range(len_splited_sent)]\n",
    "    \n",
    "    ## STEP1 - 출력확률 2D-State에 해당 tag와 word에 맞게 값을 채워넣는다.\n",
    "    # 각 state에 대해 모든 출력확률을 구한다.\n",
    "    for i, word in enumerate(splited_sent):\n",
    "        for j, tag in enumerate(set_tag_list):\n",
    "            try:\n",
    "                output_prob_state[i][j] = output_prob_dict[(tag, word)] # 출력확률\n",
    "            except:\n",
    "                output_prob_state[i][j] = smoothing\n",
    "    \n",
    "    #print(set_tag_list)     \n",
    "    #print(output_prob_state)\n",
    "\n",
    "    ## STEP2 - 통합확률 bottom-up process (dynamic programming)\n",
    "    # 단어축-state가 진행될 때마다 차차 확률을 곱해나가고 이전 시간의 태그축-state index를 저장\n",
    "    # 단어축: 시간의 개념, 태그축: 공간의 개념\n",
    "    for i, word in enumerate(splited_sent):\n",
    "        for j, tag in enumerate(set_tag_list):\n",
    "            if i == 0: # 첫 단어에 대해서만...\n",
    "                start_state = 1.0 # 첫 단어 기준으로 이전 통합확률은 1로 명시함 (왜냐? 존재하질 않으므로).\n",
    "                try:\n",
    "                    cur_trans_prob = trans_prob_dict['<s>', set_tag_list[j]] # 첫 단어는 무조건 <s>가 이전 태그임.\n",
    "                except:\n",
    "                    cur_trans_prob = smoothing\n",
    "                # 통합확률 bottom-up process\n",
    "                total_prob_state[i][j] = start_state * cur_trans_prob *  output_prob_state[i][j] # 첫 단어에서의 통합확률\n",
    "                total_prev_state_idx[i][j] = set_tag_list.index('<s>') # 첫 단어에서의 이전index\n",
    "            else:\n",
    "                # ExtractMaxProb 함수를 통하여 가장 확률이 높은 이전 시간의 태그축-state index를 선택하고 현재 2D-state의 통합확률로 할당\n",
    "                # 여기서 max 확률과 그의 이전index를 저장하는 작업이 dynamic programming의 핵심 \n",
    "                total_prob_state[i][j], total_prev_state_idx[i][j] = ExtractMax_Prob_Index(total_prob_state, i, j, trans_prob_dict, output_prob_state)\n",
    "    \n",
    "    #print(set_tag_list)\n",
    "    #for i in total_prob_state:\n",
    "        #print(i)\n",
    "        #pass\n",
    "    \n",
    "    ## STEP3 - Preparing for backtracking \n",
    "    # </s> 태그 시점에서 확률이 가장 높은 마지막 단어축-state index 선택\n",
    "    temp = [0 for x in range(len(set_tag_list))]\n",
    "    last_idx = len(splited_sent)-1\n",
    "    for j, _ in enumerate(set_tag_list):\n",
    "        end_state = 1.0 # 끝 단어 기준으로 이후 출력확률은 1로 명시함 (왜냐? 존재하질 않으므로).\n",
    "        try: \n",
    "            cur_trans_prob = trans_prob_dict[(set_tag_list[j], '</s>')]\n",
    "        except: # smoothing.\n",
    "            cur_trans_prob = smoothing\n",
    "            \n",
    "        # 마지막 통합확률 bottom-up process\n",
    "        temp[j] = total_prob_state[last_idx][j] * cur_trans_prob * end_state\n",
    "        \n",
    "    max_prob = max(temp)\n",
    "    end_state_idx = temp.index(max_prob)\n",
    "    # 이제 end_state_idx 를 기점으로 backtracking을 실시하자.\n",
    "    \n",
    "    ## STEP4 - Executing Backtracking\n",
    "    prev_tag_idx = -1\n",
    "    pos_sent = ['none_tag'] * (len(splited_sent) + 1) # <s> 태그때문에 +1해준다\n",
    "    #joint_prob = 0\n",
    "    for i, x in enumerate(reversed(pos_sent)):\n",
    "        last_idx = len(pos_sent)-1\n",
    "        ri = last_idx - i # reversed index\n",
    "        \n",
    "        if ri == last_idx: # last word\n",
    "            pos_sent[ri] = set_tag_list[end_state_idx]\n",
    "            #joint_prob = max_prob\n",
    "            prev_tag_idx = end_state_idx\n",
    "        else:\n",
    "            cur_tag_idx = total_prev_state_idx[ri][prev_tag_idx]\n",
    "            pos_sent[ri] = set_tag_list[cur_tag_idx]\n",
    "            \n",
    "            # 다음 턴을 위해서..\n",
    "            prev_tag_idx = cur_tag_idx\n",
    "            \n",
    "    print('Output:\\t',' '.join(pos_sent[1:])) # start tag는 생략\n",
    "    #print(joint_prob)\n",
    "    #print(total_prob_state)\n",
    "\n",
    "    \n",
    "#####################################################################################################\n",
    "\"\"\" Main \"\"\"\n",
    "#####################################################################################################\n",
    "for case in input_word_list:\n",
    "    print('Input:\\t', case)\n",
    "    ViterbiAlgorithm(case.split())\n",
    "    print('-----------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above results are the same as [3]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
