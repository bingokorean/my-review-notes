{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard with One-layer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function for variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weights(shape, name):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.01), name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function for model construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sharing variables using tf.variable_scope\n",
    "###  tensorboard 활용을 잘하기위해서 layer name을 쓴다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X, w_h, w_h2):\n",
    "    with tf.variable_scope(\"layer1\"):\n",
    "        h = tf.nn.relu(tf.matmul(X, w_h))\n",
    "    with tf.variable_scope(\"layer2\"):        \n",
    "        h2 = tf.nn.relu(tf.matmul(h, w_h2))\n",
    "    with tf.variable_scope(\"layer3\"):        \n",
    "        return tf.matmul(h2, w_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=False)\n",
    "X_trn, Y_trn, X_test, Y_test = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make placeholder for inputs and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.int32, [None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weight variable 와 Model 만들기\n",
    "### 첫번째 hidden layer은 input을 784 / 2 = 392 차원으로 보낸다.\n",
    "### 두번째 hidden layer은 392차원을 다시 392차원으로 보낸다.\n",
    "### output layer에서는 이를 label의 차원인 10차원으로 보낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hidden_dim = 392\n",
    "w_h = init_weights([784, hidden_dim], name = \"w_h\")\n",
    "w_h2 = init_weights([hidden_dim, hidden_dim], name = \"w_h2\")\n",
    "w_o = init_weights([hidden_dim, 10], name = \"w_o\")\n",
    "\n",
    "py_x = model(X, w_h, w_h2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight variable에 대해서 hitogram summary 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'HistogramSummary_2:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.histogram_summary(\"w_h_summ\", w_h)\n",
    "tf.histogram_summary(\"w_h2_summ\", w_h2)\n",
    "tf.histogram_summary(\"w_o_summ\", w_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"loss\"):\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(py_x, Y))\n",
    "    tf.scalar_summary(\"loss\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training operator\n",
    "#### tf.train.AdagradOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_op = tf.train.AdagradOptimizer(learning_rate=0.05).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy 계산\n",
    "### tensorflow를 통해서 accuracy를 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"accuracy\"):\n",
    "    correct = tf.nn.in_top_k(py_x, Y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    tf.scalar_summary(\"accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session initialize 및 training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SummaryWriter를 통해서 event file을 만들어서 tensorboard를 이용할 수 있게 한다.\n",
    "## merge_all_summaries() 를 이용하여 여러 summary들을 한번에 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 test accuracy 0.8761\n",
      "epoch: 1 test accuracy 0.9204\n",
      "epoch: 2 test accuracy 0.9463\n",
      "epoch: 3 test accuracy 0.9608\n",
      "epoch: 4 test accuracy 0.9666\n",
      "epoch: 5 test accuracy 0.9708\n",
      "epoch: 6 test accuracy 0.9726\n",
      "epoch: 7 test accuracy 0.9742\n",
      "epoch: 8 test accuracy 0.9748\n",
      "epoch: 9 test accuracy 0.9753\n",
      "epoch: 10 test accuracy 0.9753\n",
      "epoch: 11 test accuracy 0.9756\n",
      "epoch: 12 test accuracy 0.9763\n",
      "epoch: 13 test accuracy 0.9767\n",
      "epoch: 14 test accuracy 0.9774\n",
      "epoch: 15 test accuracy 0.9774\n",
      "epoch: 16 test accuracy 0.9776\n",
      "epoch: 17 test accuracy 0.9785\n",
      "epoch: 18 test accuracy 0.9790\n",
      "epoch: 19 test accuracy 0.9788\n",
      "epoch: 20 test accuracy 0.9787\n",
      "epoch: 21 test accuracy 0.9787\n",
      "epoch: 22 test accuracy 0.9789\n",
      "epoch: 23 test accuracy 0.9792\n",
      "epoch: 24 test accuracy 0.9794\n",
      "epoch: 25 test accuracy 0.9796\n",
      "epoch: 26 test accuracy 0.9794\n",
      "epoch: 27 test accuracy 0.9793\n",
      "epoch: 28 test accuracy 0.9792\n",
      "epoch: 29 test accuracy 0.9793\n",
      "epoch: 30 test accuracy 0.9794\n",
      "epoch: 31 test accuracy 0.9795\n",
      "epoch: 32 test accuracy 0.9797\n",
      "epoch: 33 test accuracy 0.9798\n",
      "epoch: 34 test accuracy 0.9801\n",
      "epoch: 35 test accuracy 0.9802\n",
      "epoch: 36 test accuracy 0.9800\n",
      "epoch: 37 test accuracy 0.9800\n",
      "epoch: 38 test accuracy 0.9800\n",
      "epoch: 39 test accuracy 0.9801\n",
      "epoch: 40 test accuracy 0.9801\n",
      "epoch: 41 test accuracy 0.9803\n",
      "epoch: 42 test accuracy 0.9803\n",
      "epoch: 43 test accuracy 0.9803\n",
      "epoch: 44 test accuracy 0.9802\n",
      "epoch: 45 test accuracy 0.9802\n",
      "epoch: 46 test accuracy 0.9803\n",
      "epoch: 47 test accuracy 0.9804\n",
      "epoch: 48 test accuracy 0.9804\n",
      "epoch: 49 test accuracy 0.9804\n",
      "epoch: 50 test accuracy 0.9804\n",
      "epoch: 51 test accuracy 0.9805\n",
      "epoch: 52 test accuracy 0.9804\n",
      "epoch: 53 test accuracy 0.9805\n",
      "epoch: 54 test accuracy 0.9806\n",
      "epoch: 55 test accuracy 0.9806\n",
      "epoch: 56 test accuracy 0.9806\n",
      "epoch: 57 test accuracy 0.9806\n",
      "epoch: 58 test accuracy 0.9806\n",
      "epoch: 59 test accuracy 0.9806\n",
      "epoch: 60 test accuracy 0.9806\n",
      "epoch: 61 test accuracy 0.9806\n",
      "epoch: 62 test accuracy 0.9806\n",
      "epoch: 63 test accuracy 0.9806\n",
      "epoch: 64 test accuracy 0.9805\n",
      "epoch: 65 test accuracy 0.9805\n",
      "epoch: 66 test accuracy 0.9807\n",
      "epoch: 67 test accuracy 0.9806\n",
      "epoch: 68 test accuracy 0.9806\n",
      "epoch: 69 test accuracy 0.9806\n",
      "epoch: 70 test accuracy 0.9806\n",
      "epoch: 71 test accuracy 0.9806\n",
      "epoch: 72 test accuracy 0.9806\n",
      "epoch: 73 test accuracy 0.9807\n",
      "epoch: 74 test accuracy 0.9806\n",
      "epoch: 75 test accuracy 0.9806\n",
      "epoch: 76 test accuracy 0.9806\n",
      "epoch: 77 test accuracy 0.9807\n",
      "epoch: 78 test accuracy 0.9807\n",
      "epoch: 79 test accuracy 0.9807\n",
      "epoch: 80 test accuracy 0.9807\n",
      "epoch: 81 test accuracy 0.9807\n",
      "epoch: 82 test accuracy 0.9807\n",
      "epoch: 83 test accuracy 0.9808\n",
      "epoch: 84 test accuracy 0.9808\n",
      "epoch: 85 test accuracy 0.9808\n",
      "epoch: 86 test accuracy 0.9808\n",
      "epoch: 87 test accuracy 0.9808\n",
      "epoch: 88 test accuracy 0.9808\n",
      "epoch: 89 test accuracy 0.9808\n",
      "epoch: 90 test accuracy 0.9808\n",
      "epoch: 91 test accuracy 0.9808\n",
      "epoch: 92 test accuracy 0.9808\n",
      "epoch: 93 test accuracy 0.9808\n",
      "epoch: 94 test accuracy 0.9808\n",
      "epoch: 95 test accuracy 0.9808\n",
      "epoch: 96 test accuracy 0.9809\n",
      "epoch: 97 test accuracy 0.9809\n",
      "epoch: 98 test accuracy 0.9809\n",
      "epoch: 99 test accuracy 0.9809\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "with tf.Session() as sess:    \n",
    "    writer = tf.train.SummaryWriter(\"./logs/nn_logs\", sess.graph) # for 0.8\n",
    "    merged = tf.merge_all_summaries()\n",
    "    tf.initialize_all_variables().run()\n",
    "\n",
    "    for i in range(100):\n",
    "        for start, end in zip(range(0, len(X_trn), batch_size), range(batch_size, len(X_trn)+1, batch_size)):\n",
    "            sess.run(train_op, feed_dict={X: X_trn[start:end], Y: Y_trn[start:end]})\n",
    "        summary, test_acc = sess.run([merged, accuracy], feed_dict={X: X_test, Y: Y_test})\n",
    "        writer.add_summary(summary, i) #i 번째 step\n",
    "        print(\"epoch: {} test accuracy {:0.4f}\".format(i, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## terminal 을 이용하여\n",
    "## tensorboard --logdir=/path/to/log-directory\n",
    "## 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [tensorflow]",
   "language": "python",
   "name": "Python [tensorflow]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
