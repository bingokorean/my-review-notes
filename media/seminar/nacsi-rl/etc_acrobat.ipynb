{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gym.envs.registration:Making new env: Acrobot-v1\n",
      "[2017-03-23 18:44:27,715] Making new env: Acrobot-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find old network weights\n",
      "====== Episode 1 ended with score = -620.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 2 ended with score = -920.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 3 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 4 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 5 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 6 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 7 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 8 ended with score = -525.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 9 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 10 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 11 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 12 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 13 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 14 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 15 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 16 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 17 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 18 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 19 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 20 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 21 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 22 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 23 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 24 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 25 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 26 ended with score = -601.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 27 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 28 ended with score = -727.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 29 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 30 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 31 ended with score = -898.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 32 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 33 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 34 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 35 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 36 ended with score = -831.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 37 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 38 ended with score = -731.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 39 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 40 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 41 ended with score = -793.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 42 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 43 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 44 ended with score = -882.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 45 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 46 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 47 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 48 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 49 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 50 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 51 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 52 ended with score = -784.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 53 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 54 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 55 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 56 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 57 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 58 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 59 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 60 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 61 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 62 ended with score = -1000.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 63 ended with score = -607.0, avg_loss = 0, eps = 1.0 ======\n",
      "====== Episode 64 ended with score = -1000.0, avg_loss = 207.565266578, eps = 0.1 ======\n",
      "====== Episode 65 ended with score = -1000.0, avg_loss = 202.315110169, eps = 0.1 ======\n",
      "====== Episode 66 ended with score = -1000.0, avg_loss = 176.791245406, eps = 0.1 ======\n",
      "====== Episode 67 ended with score = -1000.0, avg_loss = 190.442218066, eps = 0.1 ======\n",
      "====== Episode 68 ended with score = -770.0, avg_loss = 180.231073726, eps = 0.1 ======\n",
      "====== Episode 69 ended with score = -227.0, avg_loss = 129.017339163, eps = 0.1 ======\n",
      "====== Episode 70 ended with score = -255.0, avg_loss = 157.320737626, eps = 0.1 ======\n",
      "====== Episode 71 ended with score = -284.0, avg_loss = 227.956490021, eps = 0.1 ======\n",
      "====== Episode 72 ended with score = -212.0, avg_loss = 170.915497449, eps = 0.1 ======\n",
      "====== Episode 73 ended with score = -332.0, avg_loss = 264.695599444, eps = 0.1 ======\n",
      "====== Episode 74 ended with score = -315.0, avg_loss = 204.265735225, eps = 0.1 ======\n",
      "====== Episode 75 ended with score = -131.0, avg_loss = 160.538490772, eps = 0.1 ======\n",
      "====== Episode 76 ended with score = -384.0, avg_loss = 287.188214381, eps = 0.1 ======\n",
      "====== Episode 77 ended with score = -380.0, avg_loss = 236.730522491, eps = 0.1 ======\n",
      "====== Episode 78 ended with score = -298.0, avg_loss = 304.840664886, eps = 0.1 ======\n",
      "====== Episode 79 ended with score = -251.0, avg_loss = 244.190981335, eps = 0.1 ======\n",
      "====== Episode 80 ended with score = -1000.0, avg_loss = 247.993300111, eps = 0.1 ======\n",
      "====== Episode 81 ended with score = -1000.0, avg_loss = 218.680541048, eps = 0.1 ======\n",
      "====== Episode 82 ended with score = -1000.0, avg_loss = 167.928091326, eps = 0.1 ======\n",
      "====== Episode 83 ended with score = -1000.0, avg_loss = 129.37629435, eps = 0.1 ======\n",
      "====== Episode 84 ended with score = -1000.0, avg_loss = 122.625409758, eps = 0.1 ======\n",
      "====== Episode 85 ended with score = -1000.0, avg_loss = 97.1128384418, eps = 0.1 ======\n",
      "====== Episode 86 ended with score = -1000.0, avg_loss = 76.8058271618, eps = 0.1 ======\n",
      "====== Episode 87 ended with score = -1000.0, avg_loss = 68.0169560127, eps = 0.1 ======\n",
      "====== Episode 88 ended with score = -1000.0, avg_loss = 58.2344729767, eps = 0.1 ======\n",
      "====== Episode 89 ended with score = -1000.0, avg_loss = 50.2638949451, eps = 0.1 ======\n",
      "====== Episode 90 ended with score = -1000.0, avg_loss = 41.4550668964, eps = 0.1 ======\n",
      "====== Episode 91 ended with score = -1000.0, avg_loss = 37.0023570919, eps = 0.1 ======"
     ]
    }
   ],
   "source": [
    "'''\n",
    "A DQN model to solve Acrobot problem.\n",
    "Based on http://www.nervanasys.com/demystifying-deep-reinforcement-learning/\n",
    "Implemented by Li Bin\n",
    "'''\n",
    "\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "\n",
    "OUT_DIR = 'acro2' # default saving directory\n",
    "MAX_SCORE_QUEUE_SIZE = 100  # number of episode scores to calculate average performance\n",
    "GAME = 'Acrobot-v1'    # name of game\n",
    "TIMESTEP_LIMIT = 1000   # Time step limit of each episode\n",
    "\n",
    "# Parameters\n",
    "class options:\n",
    "    def __init__(self):\n",
    "        self.MAX_EPISODE = 10000\n",
    "        self.ACTION_DIM = 3\n",
    "        self.OBSERVATION_DIM=6\n",
    "        self.GAMMA = 0.9\n",
    "        self.INIT_EPS = 1.0\n",
    "        self.FINAL_EPS = 1e-5\n",
    "        self.EPS_DECAY = 0.1\n",
    "        self.EPS_ANNEAL_STEPS = 60000\n",
    "        self.LR =1e-4\n",
    "        self.MAX_EXPERIENCE = 60000\n",
    "        self.BATCH_SIZE = 512\n",
    "        self.H1_SIZE = 128\n",
    "        self.H2_SIZE = 128\n",
    "        self.H3_SIZE = 128\n",
    "        \n",
    "def get_options():    \n",
    "    option=options()\n",
    "    return option\n",
    "\n",
    "\n",
    "'''\n",
    "The DQN model itself.\n",
    "Remain unchanged when applied to different problems.\n",
    "'''\n",
    "class QAgent:\n",
    "    \n",
    "    # A naive neural network with 3 hidden layers and relu as non-linear function.\n",
    "    def __init__(self, options):\n",
    "        self.W1 = self.weight_variable([options.OBSERVATION_DIM, options.H1_SIZE])\n",
    "        self.b1 = self.bias_variable([options.H1_SIZE])\n",
    "        self.W2 = self.weight_variable([options.H1_SIZE, options.H2_SIZE])\n",
    "        self.b2 = self.bias_variable([options.H2_SIZE])\n",
    "        self.W3 = self.weight_variable([options.H2_SIZE, options.H3_SIZE])\n",
    "        self.b3 = self.bias_variable([options.H3_SIZE])\n",
    "        self.W4 = self.weight_variable([options.H3_SIZE, options.ACTION_DIM])\n",
    "        self.b4 = self.bias_variable([options.ACTION_DIM])\n",
    "    \n",
    "    # Weights initializer\n",
    "    def xavier_initializer(self, shape):\n",
    "        dim_sum = np.sum(shape)\n",
    "        if len(shape) == 1:\n",
    "            dim_sum += 1\n",
    "        bound = np.sqrt(6.0 / dim_sum)\n",
    "        return tf.random_uniform(shape, minval=-bound, maxval=bound)\n",
    "\n",
    "    # Tool function to create weight variables\n",
    "    def weight_variable(self, shape):\n",
    "        return tf.Variable(self.xavier_initializer(shape))\n",
    "\n",
    "    # Tool function to create bias variables\n",
    "    def bias_variable(self, shape):\n",
    "        return tf.Variable(self.xavier_initializer(shape))\n",
    "\n",
    "    # Add options to graph\n",
    "    def add_value_net(self, options):\n",
    "        observation = tf.placeholder(tf.float32, [None, options.OBSERVATION_DIM])\n",
    "        h1 = tf.nn.relu(tf.matmul(observation, self.W1) + self.b1)\n",
    "        h2 = tf.nn.relu(tf.matmul(h1, self.W2) + self.b2)\n",
    "        h3 = tf.nn.relu(tf.matmul(h2, self.W3) + self.b3)\n",
    "        Q = tf.squeeze(tf.matmul(h3, self.W4) + self.b4)\n",
    "        return observation, Q\n",
    "\n",
    "    # Sample action with random rate eps\n",
    "    def sample_action(self, Q, feed, eps, options):\n",
    "        if random.random() <= eps:\n",
    "            action_index = env.action_space.sample()\n",
    "        else:\n",
    "            act_values = Q.eval(feed_dict=feed)\n",
    "            action_index = np.argmax(act_values)\n",
    "        action = np.zeros(options.ACTION_DIM)\n",
    "        action[action_index] = 1\n",
    "        return action\n",
    "\n",
    "\n",
    "\n",
    "def train(env):\n",
    "    \n",
    "    # Define placeholders to catch inputs and add options\n",
    "    options = get_options()\n",
    "    agent = QAgent(options)\n",
    "    config = tf.ConfigProto(log_device_placement=True)\n",
    "    config.gpu_options.per_process_gpu_memory_fraction=0.1 # don't hog all vRAM\n",
    "    sess = tf.InteractiveSession(\"\", config=config)\n",
    "    \n",
    "    obs, Q1 = agent.add_value_net(options)\n",
    "    act = tf.placeholder(tf.float32, [None, options.ACTION_DIM])\n",
    "    rwd = tf.placeholder(tf.float32, [None, ])\n",
    "    next_obs, Q2 = agent.add_value_net(options)\n",
    "    \n",
    "    values1 = tf.reduce_sum(tf.mul(Q1, act), reduction_indices=1)\n",
    "    values2 = rwd + options.GAMMA * tf.reduce_max(Q2, reduction_indices=1)\n",
    "    loss = tf.reduce_mean(tf.square(values1 - values2))\n",
    "    train_step = tf.train.AdamOptimizer(options.LR).minimize(loss)\n",
    "    \n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    \n",
    "    # saving and loading networks\n",
    "    saver = tf.train.Saver()\n",
    "    checkpoint = tf.train.get_checkpoint_state(\"check-acro2\")\n",
    "    if checkpoint and checkpoint.model_checkpoint_path:\n",
    "        saver.restore(sess, checkpoint.model_checkpoint_path)\n",
    "        print(\"Successfully loaded:\", checkpoint.model_checkpoint_path)\n",
    "    else:\n",
    "        print(\"Could not find old network weights\")\n",
    "    \n",
    "    # Some initial local variables\n",
    "    feed = {}\n",
    "    eps = options.INIT_EPS\n",
    "    global_step = 0\n",
    "    exp_pointer = 0\n",
    "    learning_finished = False\n",
    "    \n",
    "    # The replay memory\n",
    "    obs_queue = np.empty([options.MAX_EXPERIENCE, options.OBSERVATION_DIM])\n",
    "    act_queue = np.empty([options.MAX_EXPERIENCE, options.ACTION_DIM])\n",
    "    rwd_queue = np.empty([options.MAX_EXPERIENCE])\n",
    "    next_obs_queue = np.empty([options.MAX_EXPERIENCE, options.OBSERVATION_DIM])\n",
    "    \n",
    "    # Score cache\n",
    "    score_queue = []\n",
    "\n",
    "    for i_episode in xrange(options.MAX_EPISODE):\n",
    "        \n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "        sum_loss_value = 0\n",
    "        epi_step = 0\n",
    "        \n",
    "        while not done:\n",
    "            global_step += 1\n",
    "            epi_step += 1\n",
    "            if global_step % options.EPS_ANNEAL_STEPS == 0 and eps > options.FINAL_EPS:\n",
    "                eps = eps * options.EPS_DECAY\n",
    "            #env.render()\n",
    "            \n",
    "            obs_queue[exp_pointer] = observation\n",
    "            action = agent.sample_action(Q1, {obs : np.reshape(observation, (1, -1))}, eps, options)\n",
    "            act_queue[exp_pointer] = action\n",
    "            observation, reward, done, _ = env.step(np.argmax(action))\n",
    "            \n",
    "            score += reward\n",
    "            reward += score / 100 # Reward will be the accumulative score divied by 100\n",
    "            \n",
    "            if done and epi_step < TIMESTEP_LIMIT:\n",
    "                reward = 1000 # If make it, send a big reward\n",
    "                observation = np.zeros_like(observation)\n",
    "            \n",
    "            rwd_queue[exp_pointer] = reward\n",
    "            next_obs_queue[exp_pointer] = observation\n",
    "    \n",
    "            exp_pointer += 1\n",
    "            if exp_pointer == options.MAX_EXPERIENCE:\n",
    "                exp_pointer = 0 # Refill the replay memory if it is full\n",
    "    \n",
    "            if global_step >= options.MAX_EXPERIENCE:\n",
    "                rand_indexs = np.random.choice(options.MAX_EXPERIENCE, options.BATCH_SIZE)\n",
    "                feed.update({obs : obs_queue[rand_indexs]})\n",
    "                feed.update({act : act_queue[rand_indexs]})\n",
    "                feed.update({rwd : rwd_queue[rand_indexs]})\n",
    "                feed.update({next_obs : next_obs_queue[rand_indexs]})\n",
    "                if not learning_finished:   # If not solved, we train and get the step loss\n",
    "                    step_loss_value, _ = sess.run([loss, train_step], feed_dict = feed)\n",
    "                else:   # If solved, we just get the step loss\n",
    "                    step_loss_value = sess.run(loss, feed_dict = feed)\n",
    "                # Use sum to calculate average loss of this episode\n",
    "                sum_loss_value += step_loss_value\n",
    "    \n",
    "        print \"====== Episode {} ended with score = {}, avg_loss = {}, eps = {} ======\".format(i_episode+1, score, sum_loss_value / epi_step, eps)\n",
    "        score_queue.append(score)\n",
    "        if len(score_queue) > MAX_SCORE_QUEUE_SIZE:\n",
    "            score_queue.pop(0)\n",
    "            if np.mean(score_queue) > -100:  # The threshold of being solved\n",
    "                learning_finished = True\n",
    "            else:\n",
    "                learning_finished = False\n",
    "        if learning_finished:\n",
    "            print \"Testing !!!\"\n",
    "        # save progress every 100 episodes\n",
    "        if learning_finished and i_episode % 100 == 0:\n",
    "            saver.save(sess, 'check-acro2/' + GAME + '-dqn', global_step = global_step)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(GAME)\n",
    "    env.spec.timestep_limit = TIMESTEP_LIMIT\n",
    "    #env = gym.wrappers.Monitor(env, OUT_DIR,force=True)\n",
    "    train(env)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
