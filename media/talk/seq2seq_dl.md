# Sequence to Sequence Deep Learning

Deep Learning School on September 24/25, 2016 <br>
Quoc Le, Google

* Note for this talk is as follow: [**note**](https://1drv.ms/w/s!AllPqyV9kKUrg2WDGoMKFcyOvZ4F)
* My presentation based on this talk is as follows: [**note**](https://1drv.ms/p/s!AllPqyV9kKUrhEsRUjDs4t7sXqwJ)

## Summary
* Bag-of-Word reprsentation
* Recurrent neural representation
* Sequence to sequence model
   * scheduled sampling
   * beam search decoding
* Sequence to sequence with attention mechanism
   * weighted averaged for whole hidden nodes as additional signal
* LSTMCell
   * add memory cell
   * sigmoid: controller
 Â  * tanh: nonlinear distortion
* RNNs with augmented Memory
* RNNs with augmented operation
