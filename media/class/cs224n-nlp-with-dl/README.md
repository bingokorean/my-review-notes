# CS224n: Natural Language Processing with Deep Learning

* Stanford CS224N
* Winter 2019
* [Schedule](http://web.stanford.edu/class/cs224n/index.html#schedule)
* [Videos](https://www.youtube.com/playlist?list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z)

강의를 듣고 이해한 내용과 저의 생각을 slide의 주석으로 정리했습니다.


## Contents

1. Introduction and Word Vectors [[annotated-slide](https://www.dropbox.com/s/2msvergk247vepa/cs224n-2019-lecture01-wordvecs1_gritmind.pdf?dl=0)]
2. Word Vectors 2 and Word Senses [[annotated-slide](https://www.dropbox.com/s/ljifhzww31fxj05/cs224n-2019-lecture02-wordvecs2_gritmind.pdf?dl=0)]
3. Word Window Classification, Neural Networks, and Maxtrix Calculus [[annotated-slide](https://www.dropbox.com/s/h17ia9rqm6x4fr0/cs224n-2019-lecture03-neuralnets_gritmind.pdf?dl=0)]
4. Backpropagation and Computation Graphs [[annotated-slide](https://www.dropbox.com/s/72s4nugvrzcmvuh/cs224n-2019-lecture04-backprop_gritmind.pdf?dl=0)]
5. Linguistic Structure: Dependency Parsing [[annotated-slide](https://www.dropbox.com/s/u5o7ohc6fi7nyh3/cs224n-2019-lecture05-dep-parsing_gritmind.pdf?dl=0)]
6. The probability of a sentence? Recurrent Neural Networks and Language Models [[annotated-slide](https://www.dropbox.com/s/y7ydxsqdkaiovfc/cs224n-2019-lecture06-rnnlm_grtimind.pdf?dl=0)]
7. Vanishing Gradients and Fancy RNNs [[annotated-slide](https://www.dropbox.com/s/pwwu5s8szuxup1s/cs224n-2019-lecture07-fancy-rnn_gritmind.pdf?dl=0)]
8. Machine Translation, Seq2Seq and Attention [[annotated-slide](https://www.dropbox.com/s/t2ls3idsrcbx0wf/cs224n-2019-lecture08-nmt_gritmind.pdf?dl=0)]
9. Practical Tips for Final Projects [[annotated-slide](https://www.dropbox.com/s/leizs7i17dyilom/cs224n-2019-lecture09-final-projects_gritmind.pdf?dl=0)]
10. Question Answering and the Default Final Project [[annotated-slide](https://www.dropbox.com/s/psmshhscejadb8w/cs224n-2019-lecture10-QA_gritmind.pdf?dl=0)]
11. ConvNets for NLP [[annotated-slide](https://www.dropbox.com/s/s7fnmdutidzl0x4/cs224n-2019-lecture11-convnets_gritmind.pdf?dl=0)]
12. Information from parts of words: Subword Models [[annotated-slide](https://www.dropbox.com/s/8cblmw2y006cqaj/cs224n-2019-lecture12-subwords_gritmind.pdf?dl=0)]
13. Modeling contexts of use: Contextual Representations and Pretraining [[annotated-slide](https://www.dropbox.com/s/y46jbvxlkgo7te0/cs224n-2019-lecture13-contextual-representations_gritmind.pdf?dl=0)]
14. Transformers and Self-Attention For Generative Models [[annotated-slide](https://www.dropbox.com/s/uamtc4uvunog5s8/cs224n-2019-lecture14-transformers_gritmind.pdf?dl=0)]
15. Natural Language Generation
16. Reference in Language and Coreference Resolution
17. Multitask Learning: A general model for NLP?
18. Constituency Parsing and Tree Recursive Neural Networks
19. Safety, Bias, and Fairness
20. Future of NLP + Deep Learning
